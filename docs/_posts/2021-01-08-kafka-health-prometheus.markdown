---
layout: post
title:  "Kafka Broker Health Checks and Prometheus"
date:   2021-01-08 00:02:06 +0200
---
Our long running Kafka consumers were suddenly starting to time out on metadata requests from the broker. Why? and why now?

Digging deeper, we saw that exactly every 10 seconds a Kafka consumer was spawned. It had what appeared to be a random Group ID number. It then subscribed to a fixed topic and we never heard from it again. We tried only running the consuming services along with the Kafka broker, leaving every other service down. This stopped the random-groupID-consumers from spawning. We realised that starting our Prometheus server causes the consumers to respawn. That didn't make any sense to me, our Prometheus server just scrapes some REST endpoints. The scraped services are those who are responsible to serve the requests with metrics.

Our logs showed that the random-groupID-consumers were created in a class responsible to check the connectivity to the Kafka broker. The way we tested the connection was trying to call `poll()` using some consumer. But we didn't want to steal messages from real consumers, so we just used a random group ID. This is problematic in a few ways. For start, if we don't want to steal events, it suffices to just create a dedicated group for health checks. Second, we don't really have to create a new consumer everytime, we can just create one and use it for the entire lifetime of the application, just like we re-use the same HTTP client instance for all REST calls. (I'm referring to consumer and client in an abstract sense, as class instances. It's obviously a bad idea to re-use the same network connection for a long time. Always check the implementation of your clients/consumers). But lastly, even if we did need to generate a new group everytime for some reason, using just a GUID as the sole identifier makes debugging a nightmare. A much better (random) group identifier would be something like `"kafka-broker-health-check-$GUID"`.

Using a single dedicated health check consumer solved our problems. The Kafka broker's offset files were getting filled with garbage due to the large number of groups it had to keep track of. This made serving real consumers slow to the point of time out.

Back to the Prometheus question: How come stopping the Prometheus server stopped the health checks? 
Prometheus is a pull based service. The application exposes the metrics and keeps them updated, and Promehteus reads the data periodically. The [Java SDK](https://github.com/prometheus/client_java) provides metric classes like Gauge and Histogram. The application instantiates them and then registeres them with the `CollectorRegistry`. Now its up to the app to only update the values, at it's own time, asynchronously to the scraping. This de-coupling has its pros and cons.

Folllowing this logic why the application is affected by scraping it's metrics? It shouldn't. Yet we still observed that stopping the scraping stops the consumers creation. This is because we didn't use the standard Prometheus SDK for our health reporting metrics. Instead we used [Dropwizard](https://www.dropwizard.io/en/latest/)'s metric objects. Albeit being a sperate project, Prometheus' registry supports Dropwziard metrics. Supporting, in this case, translates to "being able to parse" and not to "folllowing the same design principles": Dropwizard metrics are not value wrappers like Prometheus' metrics, they are functional interfaces that provide `getValue()`. In our case getting the value meant checking connectivity, which in turn meant: consumer creation.

I find it necessary to point out that it is still possible to use Dropwziard metrics and to follow Prometheus' design principles. We would just let `getValue()` read from a registry rather than do a computation. But I'm sure you can see how silly this becomes now that we have a registry reading from another registry rather than just pushing everything to the same place.

So we answered the "why", but what about the "why *now*"? What has changed? The answer is [KIP-500](https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum), the gradual weakening of the dependency between ZooKeeper and Kafka, rolling out on each new release. Kafka uses ZooKeeper to store configuration, offsets and paritions data. KIP-500's goal is to make Kafka store that data internally. In older versions, when ZooKeeper stored most of this data it could handle all the garbage we threw at it. Now that Kafka stores more and more internally it can't keep up.

